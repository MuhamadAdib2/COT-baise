# Chain of Thought Reasoning Experiment

This project explores the effect of Chain of Thought (CoT) reasoning on a language model’s performance. The experiments involve solving math problems and handling sensitive prompts. Ethical guardrails are also implemented to prevent harmful responses.

## Overview
The project tests whether CoT reasoning improves the accuracy of language model outputs, especially for sensitive and unethical requests. An ethical intervention message is used when the model encounters harmful prompts.

## Usage
1. Install dependencies listed in `requirements.txt`.
2. Run the Jupyter notebook `Alhaj_Ibrahim.ipynb` to execute the experiments.

## Results
- **Math Task**: CoT reasoning improves the model’s problem-solving abilities.
- **Sensitive Task**: Ethical guardrails successfully prevent inappropriate responses.

## License
This project is licensed under the MIT License.
